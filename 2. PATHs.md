Ok, you got it. Right? To be able to have more sites on a server/IP I need to have a `Host:` header to tell I'm using an extension of the old HTTP version 1.0. But hey, all I'm getting is a bunch of HTML. I should have conceptually wrote this in the previous chapter/file... but this is how my mind works. I hope I did not lost you in the thinking.

When you get the HTML back, there are informations/URLs of the resources that compose the page. Images, CSS and more elements. 

PAUSE: (Jargon). A URL is a complete address of a resource over the internet. A file or a service. Unique resource location... you can search for it. Images, that's self explanatory. CSS are the style sheets... to make your page look fancy.

How do I get all these elements from a site? Please, don't use a telnet. You can, but...
Let's look at what we got back from www.example.com -> nope, it has no resources. 
Let's look at... please just right click copy a logo from a web page you are browsing.

https://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png
(ok, by just making this example I realized there is no much change to avoid F12)

Let's look at that URL... we have the protocol, in this case it's https, on the www.wikipedia.org domain... then comes the PATH /portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png
That is what denotes the position of a file on a webserver. From an internet user perspective.

Let's see if wikipedia answers on telnet, just for academic purpose... 

```
% telnet www.wikipedia.org 80
Trying 2a02:ec80:300:ed1a::1...
Connected to dyna.wikimedia.org.
Escape character is '^]'.
GConnection closed by foreign host.
```
Yes, and it kicks me out because I'm not fast enough on typing. What I intended to do was to:

`GET /portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png HTTP/1.1`
`Host: www.wikipedia.org`

Most likely I would be forced to SSL... we can to that, I still wanna get u playing and digest curl propely... so, let's do it... we only have to force the request as http... and see.

`curl http://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png`
nothing comes back... ok, `-v` will tell us more... 

```
curl -v http://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png
* Host www.wikipedia.org:80 was resolved.
* IPv6: 2a02:ec80:300:ed1a::1
* IPv4: 185.15.59.224
*   Trying [2a02:ec80:300:ed1a::1]:80...
* Connected to www.wikipedia.org (2a02:ec80:300:ed1a::1) port 80
> GET /portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png HTTP/1.1
> Host: www.wikipedia.org
> User-Agent: curl/8.7.1
> Accept: */*
> 
* Request completely sent off
< HTTP/1.1 301 Moved Permanently
< content-length: 0
< location: https://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png
< server: HAProxy
< x-cache: cp3067 int
< x-cache-status: int-tls
< connection: close
< 
* Closing connection
```
We are being told, look... the resource is somewhere else `HTTP/1.1 301 Moved Permanently`
where? here! `location: https://www.wikipedia.org/portal/wikipedia.org/assets/img/Wikipedia-logo-v2.png`

yeah, nice, but how do I get it? `-L` follow the location header... I'll avoid to paste the output... but of course it is telling us we should have know, and provided a way to write the file

```
Warning: Binary output can mess up your terminal. Use "--output -" to tell 
Warning: curl to output it to your terminal anyway, or consider "--output 
Warning: <FILE>" to save to a file.
```

You can provide the info, or simply use a tool dedicated to that `wget`.

Anyway, what you got from this chapter and the previous one is that any resource on the internet is provided over http given a `Host:` and a `PATH`. These are the 2 basic informations that any reverse proxy requires to serve you properly.

Proceed to chapter 3. SSL if you want, or skip forward.
(I'm going as my mind goes... I'm already thinking of SNI & CAs, but... let's hold the horses)
